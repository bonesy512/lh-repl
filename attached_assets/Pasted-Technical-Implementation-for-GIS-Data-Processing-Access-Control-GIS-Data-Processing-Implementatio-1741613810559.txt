Technical Implementation for GIS Data Processing & Access Control
GIS Data Processing Implementation
Setup Processing Pipeline:

Copy snippet
import geopandas as gpd
import fiona
from shapely.geometry import shape
import databutton as db
import json
import re

def process_geodatabase(gdb_path):
    # Open geodatabase and list layers
    layers = fiona.listlayers(gdb_path)
    
    # Process each layer
    for layer_name in layers:
        # Read layer as GeoDataFrame
        gdf = gpd.read_file(gdb_path, layer=layer_name)
        
        # Convert to GeoJSON for storage
        geojson_data = json.loads(gdf.to_json())
        
        # Store in Databutton storage with sanitized key
        sanitized_key = re.sub(r'[^a-zA-Z0-9._-]', '', f"gis_layer_{layer_name}")
        db.storage.json.put(sanitized_key, geojson_data)
        
        # Create simplified version for non-premium users
        if layer_name == "parcels":  # Example core layer
            simplified = create_simplified_layer(gdf)
            db.storage.json.put(f"{sanitized_key}_simplified", simplified)
Create MapBox Tileset Configuration:

Copy snippet
def create_tileset_config(layer_name, is_premium=False):
    # Configuration for MapBox tilesets
    tileset_config = {
        "version": 1,
        "layers": {
            f"{layer_name}": {
                "source": f"mapbox://tileset-source/your-username/{layer_name}",
                "minzoom": 10,
                "maxzoom": 16
            }
        },
        "attribution": "Your GIS data attribution"
    }
    
    # Store tileset configuration
    key = f"tileset_config_{layer_name}"
    if is_premium:
        key += "_premium"
    db.storage.json.put(key, tileset_config)
Backend API for Layer Access:

Copy snippet
from fastapi import APIRouter, Depends
from app.auth import AuthorizedUser
from pydantic import BaseModel

router = APIRouter()

class GisLayerRequest(BaseModel):
    layer_name: str
    bbox: list[float]  # [west, south, east, north]

class GisLayerResponse(BaseModel):
    features: list
    layer_type: str
    attribution: str

@router.post("/gis-layer")
async def get_gis_layer(request: GisLayerRequest, user: AuthorizedUser = None):
    # Determine if user has premium access
    has_premium = False
    if user:
        user_profile = await get_user_profile(user.sub)
        has_premium = user_profile.get("subscriptionStatus") == "active"
    
    # Get appropriate layer based on access level
    layer_key = f"gis_layer_{request.layer_name}"
    if not has_premium:
        layer_key += "_simplified"
    
    # Get layer data from storage
    try:
        layer_data = db.storage.json.get(layer_key)
        
        # Filter by bounding box for efficiency
        filtered_features = filter_by_bbox(layer_data["features"], request.bbox)
        
        return GisLayerResponse(
            features=filtered_features,
            layer_type=layer_data.get("type", "FeatureCollection"),
            attribution="Your GIS data attribution"
        )
    except Exception as e:
        return {"error": f"Layer not found or access denied: {str(e)}"}
On-Demand Processing Implementation
Premium Data Queue System:

Copy snippet
from pydantic import BaseModel
import uuid
import time

class PremiumDataRequest(BaseModel):
    parcel_id: str
    request_type: str  # "ownership", "valuation", "history", etc.

class PremiumDataResponse(BaseModel):
    job_id: str
    status: str
    data: dict = None

@router.post("/request-premium-data")
async def request_premium_data(request: PremiumDataRequest, user: AuthorizedUser):
    # Check user subscription
    user_profile = await get_user_profile(user.sub)
    if user_profile.get("subscriptionStatus") != "active":
        return {"error": "Premium subscription required"}
    
    # Create job ID
    job_id = str(uuid.uuid4())
    
    # Store job in Firestore
    job_data = {
        "user_id": user.sub,
        "parcel_id": request.parcel_id,
        "request_type": request.request_type,
        "status": "queued",
        "created_at": time.time(),
        "data": None
    }
    
    await db.firestore.collection("premium_data_jobs").document(job_id).set(job_data)
    
    # Trigger processing (could be done by a background worker)
    # For demo, we'll just update the status immediately
    process_premium_data_job(job_id)
    
    return PremiumDataResponse(job_id=job_id, status="queued")
Job Processing Function:

Copy snippet
async def process_premium_data_job(job_id):
    # Get job details
    job_ref = db.firestore.collection("premium_data_jobs").document(job_id)
    job = await job_ref.get()
    job_data = job.to_dict()
    
    # Update status
    await job_ref.update({"status": "processing"})
    
    try:
        # Process based on request type
        if job_data["request_type"] == "ownership":
            # Fetch ownership data from GIS files
            data = fetch_ownership_data(job_data["parcel_id"])
        elif job_data["request_type"] == "valuation":
            # Fetch valuation data
            data = fetch_valuation_data(job_data["parcel_id"])
        else:
            data = {"error": "Unknown request type"}
        
        # Update job with results
        await job_ref.update({
            "status": "completed",
            "data": data,
            "completed_at": time.time()
        })
    except Exception as e:
        # Handle errors
        await job_ref.update({
            "status": "error",
            "error": str(e),
            "completed_at": time.time()
        })
Frontend Integration:

Copy snippet
// React hook for premium data
const usePremiumData = (parcelId: string, dataType: string) => {
  const [data, setData] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const { user } = useUserGuardContext();
  
  useEffect(() => {
    if (!user || !parcelId) return;
    
    const fetchPremiumData = async () => {
      setLoading(true);
      try {
        // Request premium data processing
        const response = await brain.request_premium_data({
          parcel_id: parcelId,
          request_type: dataType
        });
        
        const { job_id } = await response.json();
        
        // Poll for job completion
        const checkJobStatus = async () => {
          const statusResponse = await brain.check_job_status({
            job_id
          });
          const job = await statusResponse.json();
          
          if (job.status === 'completed') {
            setData(job.data);
            setLoading(false);
          } else if (job.status === 'error') {
            setError(job.error);
            setLoading(false);
          } else {
            // Continue polling
            setTimeout(checkJobStatus, 2000);
          }
        };
        
        checkJobStatus();
      } catch (err) {
        setError(err.message);
        setLoading(false);
      }
    };
    
    fetchPremiumData();
  }, [parcelId, dataType, user]);
  
  return { data, loading, error };
};
This implementation combines efficient upfront processing with on-demand access to premium data, creating a scalable system that:

Preserves the integrity of your geospatial data
Effectively enforces your subscription-based access control
Optimizes performance for both free and premium users
Integrates seamlessly with your existing Firebase authentication
Would you like more details on any specific aspect of this implementation?